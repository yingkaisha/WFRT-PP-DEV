{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys tools\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "# data tools\n",
    "import h5py\n",
    "import zarr\n",
    "import pygrib\n",
    "import numpy as np\n",
    "#import numba as nb\n",
    "from datetime import datetime\n",
    "\n",
    "# custom tools\n",
    "#sys.path.insert(0, '/glade/u/home/ksha/WORKSPACE/utils/')\n",
    "#sys.path.insert(0, '/glade/u/home/ksha/WORKSPACE/Analog_BC/')\n",
    "#sys.path.insert(0, '/glade/u/home/ksha/WORKSPACE/Analog_BC/utils/')\n",
    "#import data_utils as du\n",
    "#import analog_utils as ana\n",
    "\n",
    "sys.path.insert(0, '/glade/u/home/ksha/PUBLISH/WFRT-PP-DEV/')\n",
    "sys.path.insert(0, '/glade/u/home/ksha/PUBLISH/WFRT-PP-DEV/libs/')\n",
    "\n",
    "import nonDL_lib as nDL\n",
    "import utils\n",
    "from namelist_casper import * \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Function ends -------- #\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('year_fcst', help='year_fcst')\n",
    "# parser.add_argument('part', help='part')\n",
    "# args = vars(parser.parse_args())\n",
    "# year_fcst = int(args['year_fcst'])\n",
    "# part_ = int(args['part'])\n",
    "\n",
    "EN = 25\n",
    "\n",
    "dt_utc_now = datetime.utcnow()\n",
    "dt_fmt_string = datetime.strftime(dt_utc_now, '%Y%m%d')\n",
    "dt_day_of_year = dt_utc_now.timetuple().tm_yday\n",
    "dt_month_from_zero = dt_utc_now.month-1\n",
    "flag_leap_year = utils.leap_year_checker(dt_utc_now.year)\n",
    "\n",
    "# ---------------------------------- #\n",
    "\n",
    "# arg2\n",
    "LEADs = np.arange(0, 54, dtype=np.int)\n",
    "N_leads = len(LEADs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_analog = np.arange(2000, 2019)\n",
    "\n",
    "# importing domain information\n",
    "with h5py.File(save_dir+'BC_domain_info.hdf', 'r') as h5io:\n",
    "    lat_bc = h5io['bc_lat'][...] # lats of the BC domain\n",
    "    lon_bc = h5io['bc_lon'][...] # lons of the BC domain\n",
    "    land_mask_bc = h5io['land_mask_bc'][...] # selecting OCEAN grids from the BC domain\n",
    "\n",
    "ocean_mask_bc = np.logical_not(land_mask_bc) # selecting LAND grids from the BC domain\n",
    "\n",
    "bc_shape = land_mask_bc.shape\n",
    "N_grids = np.sum(ocean_mask_bc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supplemental locations\n",
    "SL_xy_dict = {}\n",
    "with h5py.File(save_dir+'SL20_d4_unique.hdf', 'r') as h5io:\n",
    "    #IxIy_unique = h5io['unique_inds'][...]\n",
    "    for i in range(12):\n",
    "        temp = h5io['mon_{}_inds'.format(i)][...]\n",
    "        temp = temp.astype(int)\n",
    "        SL_xy_dict['{}'.format(i)] = temp\n",
    "#IxIy_unique = IxIy_unique.astype(int)\n",
    "SL_xy = tuple(SL_xy_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "AnEn_out = np.empty((N_leads, N_grids, EN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing lead time = 0\n",
      "Processing lead time = 1\n",
      "Processing lead time = 2\n",
      "Processing lead time = 3\n",
      "Processing lead time = 4\n",
      "Processing lead time = 5\n",
      "Processing lead time = 6\n",
      "Processing lead time = 7\n",
      "Processing lead time = 8\n",
      "Processing lead time = 9\n",
      "Processing lead time = 10\n",
      "Processing lead time = 11\n",
      "Processing lead time = 12\n",
      "Processing lead time = 13\n",
      "Processing lead time = 14\n",
      "Processing lead time = 15\n",
      "Processing lead time = 16\n",
      "Processing lead time = 17\n",
      "Processing lead time = 18\n",
      "Processing lead time = 19\n",
      "Processing lead time = 20\n",
      "Processing lead time = 21\n",
      "Processing lead time = 22\n",
      "Processing lead time = 23\n",
      "Processing lead time = 24\n",
      "Processing lead time = 25\n",
      "Processing lead time = 26\n",
      "Processing lead time = 27\n",
      "Processing lead time = 28\n",
      "Processing lead time = 29\n",
      "Processing lead time = 30\n",
      "Processing lead time = 31\n",
      "Processing lead time = 32\n",
      "Processing lead time = 33\n",
      "Processing lead time = 34\n",
      "Processing lead time = 35\n",
      "Processing lead time = 36\n",
      "Processing lead time = 37\n",
      "Processing lead time = 38\n",
      "Processing lead time = 39\n",
      "Processing lead time = 40\n",
      "Processing lead time = 41\n",
      "Processing lead time = 42\n",
      "Processing lead time = 43\n",
      "Processing lead time = 44\n",
      "Processing lead time = 45\n",
      "Processing lead time = 46\n",
      "Processing lead time = 47\n",
      "Processing lead time = 48\n",
      "Processing lead time = 49\n",
      "Processing lead time = 50\n",
      "Processing lead time = 51\n",
      "Processing lead time = 52\n",
      "Processing lead time = 53\n"
     ]
    }
   ],
   "source": [
    "#lead = LEADs[1]\n",
    "for i, lead in enumerate(LEADs):\n",
    "    lead_int_h = int(FCSTs[lead])\n",
    "    print(\"Processing lead time = {}\".format(lead))\n",
    "    #print(\"Main program starts ...\")\n",
    "    # ------------------------------------------------- #\n",
    "    \n",
    "    # Import reforecast\n",
    "    APCP = ()\n",
    "    PWAT = ()\n",
    "    \n",
    "    for year in year_analog:\n",
    "        apcp_temp = zarr.load(BASE_dir+'BASE_APCP_year{}_lead{}.zarr'.format(year, lead))\n",
    "        pwat_temp = zarr.load(BASE_dir+'BASE_PWAT_year{}_lead{}.zarr'.format(year, lead))\n",
    "        \n",
    "        APCP += (apcp_temp,)\n",
    "        PWAT += (pwat_temp,)\n",
    "        \n",
    "    # ------------------------------------------------- #\n",
    "    \n",
    "    # Import reanalysis\n",
    "    ERA5 = ()\n",
    "    \n",
    "    for year in year_analog:\n",
    "        era_temp = zarr.load(BASE_dir+'BASE_ERA5_year{}_lead{}.zarr'.format(year, lead))\n",
    "        \n",
    "        ERA5 += (era_temp,)\n",
    "        \n",
    "    # ------------------------------------------------- #\n",
    "    \n",
    "    GEFS_dir_base = '/glade/scratch/ksha/DATA/GEFS/{}/'.format(dt_fmt_string)\n",
    "    GEFS_dir_full = GEFS_dir_base+'geavg.t00z.pgrb2s.0p25.f{0:03d}'.format(lead_int_h)\n",
    "    \n",
    "    with pygrib.open(GEFS_dir_full) as grb_io:\n",
    "        #\n",
    "        grb_reader_apcp = grb_io.select(name='Total Precipitation')[0]\n",
    "        apcp, _, _ = grb_reader_apcp.data(lat1=48.25, lat2=60.00, lon1=-141.0+360, lon2=-113.25+360)\n",
    "        apcp = np.flipud(apcp) # GEFS default: kg/m**-2 (or mm) per 3 hours\n",
    "        \n",
    "        #\n",
    "        grb_reader_pwat = grb_io.select(name='Precipitable water')[0]\n",
    "        pwat, _, _ = grb_reader_pwat.data(lat1=48.25, lat2=60.00, lon1=-141.0+360, lon2=-113.25+360)\n",
    "        pwat = np.flipud(pwat) # GEFS default: kg/m**-2 (or mm) per 3 hours\n",
    "    \n",
    "    apcp_flat = apcp[ocean_mask_bc]\n",
    "    pwat_flat = apcp[ocean_mask_bc]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    AnEn = nDL.analog_search_SL_single_day(dt_day_of_year, year_analog, apcp_flat, pwat_flat, APCP, PWAT, ERA5, EN, SL_xy, flag_leap_year)\n",
    "    #print(\"... Completed. Time = {} sec \".format((time.time() - start_time)))\n",
    "    \n",
    "    AnEn_out[i, ...] = AnEn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 2574, 25)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AnEn_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_mdss = np.arange(2010, 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Completed. Time = 165.46853280067444 sec \n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "ERA5_mdss = ()\n",
    "\n",
    "window_day = 30\n",
    "N_days = window_day*2 + 1\n",
    "\n",
    "# loop over years for MDSS training\n",
    "for year in year_mdss:\n",
    "    \n",
    "    # separate leap year\n",
    "    if utils.leap_year_checker(year):\n",
    "        flag_pick = nDL.search_nearby_days(dt_day_of_year, window=30, leap_year=True)\n",
    "    else:\n",
    "        flag_pick = nDL.search_nearby_days(dt_day_of_year, window=30, leap_year=False)\n",
    "        \n",
    "    flag_pick = flag_pick == 1\n",
    "    era_all_lead = np.empty((N_days, N_grids, N_leads))\n",
    "    \n",
    "    # loop over lead times\n",
    "    for i, lead in enumerate(LEADs):\n",
    "        era_temp = zarr.load(BASE_dir+'BC_ERA5_year{}_lead{}.zarr'.format(year, lead))\n",
    "        era_all_lead[..., i] = era_temp[flag_pick, :]\n",
    "    \n",
    "    ERA5_mdss += (era_all_lead,)\n",
    "\n",
    "ERA5_mdss = np.concatenate(ERA5_mdss)\n",
    "\n",
    "print(\"... Completed. Time = {} sec \".format((time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "AnEn_out = np.transpose(AnEn_out, (2, 0, 1))\n",
    "ERA5_mdss = np.transpose(ERA5_mdss, (0, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_dict = {'AnEn_out': AnEn_out, 'ERA5_mdss': ERA5_mdss}\n",
    "# np.save('test.npy', save_dict)\n",
    "\n",
    "# temp_data = np.load('test.npy', allow_pickle=True)[()]\n",
    "# AnEn_out = temp_data['AnEn_out']\n",
    "# ERA5_mdss = temp_data['ERA5_mdss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_pick = nDL.search_nearby_days(dt_day_of_year, window=30, leap_year=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to cut ~500 to 25 meanwhile minimizing the distribution divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CDF_fcst = CDF_estimate(AnEn_out)\n",
    "#CDF_ERA5 = CDF_estimate(ERA5_mdss)\n",
    "#record = total_divergence(CDF_ERA5, CDF_fcst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Completed. Time = 65.2543716430664 sec \n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "flag_clean, count_trial = nDL.MDSS_main(ERA5_mdss, AnEn_out, factor=5, max_trial=10000)\n",
    "print(\"... Completed. Time = {} sec \".format((time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ERA5_pick = ERA5_mdss[flag_clean][:EN, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CDF_fcst = CDF_estimate(AnEn_out)\n",
    "# CDF_ERA5 = CDF_estimate(ERA5_pick)\n",
    "# total_divergence(CDF_ERA5, CDF_fcst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(200):\n",
    "#     plt.figure()\n",
    "#     plt.plot(CDF_fcst[:, 0, i])\n",
    "#     plt.plot(CDF_ERA5[:, 0, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "AnEn_MDSS_out = nDL.schaake_shuffle(AnEn_out, ERA5_pick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 54, 2574)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AnEn_MDSS_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "anen_grid = np.empty((EN, N_leads,)+bc_shape)\n",
    "#temp_grid = np.empty(bc_shape)\n",
    "for en in range(EN):\n",
    "    for l in LEADs:\n",
    "        anen_grid[en, l, ocean_mask_bc] = AnEn_MDSS_out[en, l, :]\n",
    "anen_grid[..., land_mask_bc] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MDSS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2574, 25)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AnEn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing domain information\n",
    "# with h5py.File(save_dir+'BC_domain_info.hdf', 'r') as h5io:\n",
    "#     land_mask = h5io['land_mask_base'][...]\n",
    "#     land_mask_bc = h5io['land_mask_bc'][...]\n",
    "# bc_in_base = np.ones(land_mask.shape).astype(bool)\n",
    "# bc_in_base[bc_inds[0]:bc_inds[1], bc_inds[2]:bc_inds[3]] = land_mask_bc\n",
    "\n",
    "# # subsetting by land mask\n",
    "# bc_shape = land_mask_bc.shape\n",
    "# grid_shape = land_mask.shape\n",
    "# IND_bc = []\n",
    "# for i in range(grid_shape[0]):\n",
    "#     for j in range(grid_shape[1]):\n",
    "#         if ~bc_in_base[i, j]:\n",
    "#             IND_bc.append([i, j])\n",
    "# IND_bc = np.array(IND_bc, dtype=np.int)\n",
    "# N_grids = len(IND_bc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph tools\n",
    "import cmaps\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.mpl.geoaxes\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.io.shapereader import Reader\n",
    "from cartopy.feature import ShapelyFeature\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.collections import PatchCollection\n",
    "\n",
    "from matplotlib import ticker\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(54):\n",
    "#     plt.figure()\n",
    "#     plt.pcolormesh(lon_bc, lat_bc, anen_grid[5, i, ...], vmin=0, vmax=5, cmap=plt.cm.nipy_spectral_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fcst_grid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-9037979995e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpcolormesh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlon_bc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlat_bc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcst_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnipy_spectral_r\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'fcst_grid' is not defined"
     ]
    }
   ],
   "source": [
    "plt.pcolormesh(lon_bc, lat_bc, fcst_grid, vmin=0, vmax=5, cmap=plt.cm.nipy_spectral_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 112)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apcp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEFS_dir = '/glade/scratch/ksha/DATA/GEFS/20220114/geavg.t00z.pgrb2s.0p25.f{0:03d}'.format(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/glade/scratch/ksha/DATA/GEFS/20220114/geavg.t00z.pgrb2s.0p25.f009'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GEFS_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "    # ------------------------------------------------- #\n",
    "    print(\"AnEn search starts ...\")\n",
    "    start_time = time.time()\n",
    "    AnEn = analog_search_SL(day0, day1, year_analog, fcst_apcp, fcst_pwat, APCP, PWAT, ERA5, SL_xy, IxIy_unique, flag_leap_year)\n",
    "    print(\"... Completed. Time = {} sec \".format((time.time() - start_time)))\n",
    "    \n",
    "#     # ------------------------------------------------- #\n",
    "#     print(\"SG filter starts ...\")\n",
    "#     start_time2 = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
